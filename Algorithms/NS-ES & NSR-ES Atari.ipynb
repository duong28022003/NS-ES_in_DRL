{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"toc_visible":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Libraries & Environment"],"metadata":{"id":"DioJLvthNNAZ"}},{"cell_type":"code","source":["!pip install gym\n","!pip install gymnasium\n","!pip install ale-py==0.8.1\n","!pip install \"autorom[accept-rom-license]==0.4.2\"\n","!pip install pygame==2.1.0\n","!pip install imageio==2.36.1\n","!pip install \"imageio-ffmpeg==0.5.1\"\n","!pip install moviepy==2.1.1"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:06:28.555265Z","iopub.execute_input":"2024-12-25T11:06:28.555672Z","iopub.status.idle":"2024-12-25T11:07:50.151104Z","shell.execute_reply.started":"2024-12-25T11:06:28.555624Z","shell.execute_reply":"2024-12-25T11:07:50.149776Z"},"id":"zCkZ0-YnNNAa","outputId":"99c6def1-7f59-4e7e-877b-750329a5ab30"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gym in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym) (3.1.0)\nRequirement already satisfied: gym_notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym) (0.0.8)\nRequirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (0.29.0)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (3.1.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (0.0.4)\nCollecting ale-py==0.8.1\n  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ale-py==0.8.1) (1.26.4)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py==0.8.1) (6.4.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py==0.8.1) (4.12.2)\nDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ale-py\nSuccessfully installed ale-py-0.8.1\nCollecting autorom==0.4.2 (from autorom[accept-rom-license]==0.4.2)\n  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (4.66.4)\nCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license]==0.4.2)\n  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (2024.6.2)\nDownloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: AutoROM.accept-rom-license\n  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446667 sha256=500bc011dd88068e2fd482abe72807c7f02fa15e35a398aa018b2852c09d11bb\n  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\nSuccessfully built AutoROM.accept-rom-license\nInstalling collected packages: AutoROM.accept-rom-license, autorom\nSuccessfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\nCollecting pygame==2.1.0\n  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\nDownloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pygame\nSuccessfully installed pygame-2.1.0\nCollecting imageio==2.36.1\n  Downloading imageio-2.36.1-py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from imageio==2.36.1) (1.26.4)\nRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio==2.36.1) (10.3.0)\nDownloading imageio-2.36.1-py3-none-any.whl (315 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.4/315.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: imageio\n  Attempting uninstall: imageio\n    Found existing installation: imageio 2.34.1\n    Uninstalling imageio-2.34.1:\n      Successfully uninstalled imageio-2.34.1\nSuccessfully installed imageio-2.36.1\nCollecting imageio-ffmpeg==0.5.1\n  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio-ffmpeg==0.5.1) (70.0.0)\nDownloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: imageio-ffmpeg\nSuccessfully installed imageio-ffmpeg-0.5.1\nCollecting moviepy==2.1.1\n  Downloading moviepy-2.1.1-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: decorator<6.0,>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (5.1.1)\nRequirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (2.36.1)\nRequirement already satisfied: imageio_ffmpeg>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (0.5.1)\nRequirement already satisfied: numpy>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (1.26.4)\nCollecting proglog<=1.0.0 (from moviepy==2.1.1)\n  Downloading proglog-0.1.10-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: python-dotenv>=0.10 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (1.0.1)\nRequirement already satisfied: pillow<11.0,>=9.2.0 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (10.3.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio_ffmpeg>=0.2.0->moviepy==2.1.1) (70.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from proglog<=1.0.0->moviepy==2.1.1) (4.66.4)\nDownloading moviepy-2.1.1-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\nInstalling collected packages: proglog, moviepy\nSuccessfully installed moviepy-2.1.1 proglog-0.1.10\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions.categorical import Categorical\n","import random\n","from collections import deque\n","import time\n","import os\n","from moviepy import ImageSequenceClip\n","import cv2\n","from gym.wrappers import AtariPreprocessing, FrameStack\n","from gym.wrappers import RecordVideo\n","import numpy as np\n","from tqdm import tqdm"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:07:50.152974Z","iopub.execute_input":"2024-12-25T11:07:50.153268Z","iopub.status.idle":"2024-12-25T11:07:53.179790Z","shell.execute_reply.started":"2024-12-25T11:07:50.153237Z","shell.execute_reply":"2024-12-25T11:07:53.178940Z"},"id":"JppwDfrUNNAb"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Neural Network"],"metadata":{"id":"PpwBIzi0NNAb"}},{"cell_type":"code","source":["class NeuralNetwork(nn.Module):\n","    def __init__(self, action_space, device):\n","        super(NeuralNetwork, self).__init__()\n","\n","        self.device = device\n","\n","        self.net = nn.Sequential(\n","            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3136, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, action_space)\n","        )\n","\n","        # self.net = nn.Sequential(\n","        #     nn.Conv2d(4, 16, kernel_size=4, stride=2),\n","        #     nn.ReLU(),\n","        #     nn.Conv2d(16, 32, kernel_size=3, stride=2),\n","        #     nn.ReLU(),\n","        #     nn.Flatten(),\n","        #     nn.Linear(32 * 9 * 9, 128),\n","        #     nn.ReLU(),\n","        #     nn.Linear(128, action_space)\n","        # )\n","\n","    def forward(self, x):\n","        return self.net(x / 255.0)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:48:29.500336Z","iopub.execute_input":"2024-12-25T11:48:29.501122Z","iopub.status.idle":"2024-12-25T11:48:29.506787Z","shell.execute_reply.started":"2024-12-25T11:48:29.501088Z","shell.execute_reply":"2024-12-25T11:48:29.505881Z"},"id":"DPpszJsnNNAc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Define used functions"],"metadata":{"id":"SysvJyQRNNAc"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:07:53.189760Z","iopub.execute_input":"2024-12-25T11:07:53.190117Z","iopub.status.idle":"2024-12-25T11:07:53.423993Z","shell.execute_reply.started":"2024-12-25T11:07:53.190080Z","shell.execute_reply":"2024-12-25T11:07:53.423151Z"},"id":"mgPIHxj3NNAc","outputId":"e9040e20-53eb-4a8f-8391-aa7922c67af3"},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["def make_env(env_id, capture_video = False, seed = 1):\n","    if capture_video:\n","        env = gym.make(env_id, render_mode=\"rgb_array\")\n","        env = gym.wrappers.RecordVideo(env, \"/kaggle/working/videos\")\n","    else:\n","        env = gym.make(env_id)\n","    env = gym.wrappers.RecordEpisodeStatistics(env)\n","    env = gym.wrappers.ResizeObservation(env, (84, 84))\n","    env = gym.wrappers.GrayScaleObservation(env)\n","    env = gym.wrappers.FrameStack(env, 4)\n","    env.action_space.seed(seed)\n","    return env\n","\n","def linear_schedule(start_e, end_e, duration, t):\n","    slope = (end_e - start_e) / duration\n","    return max(slope * t + start_e, end_e)\n","\n","# Hàm khởi tạo trọng số\n","def initialize_weights(m):\n","    if isinstance(m, nn.Linear):\n","        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n","        if m.bias is not None:\n","            nn.init.zeros_(m.bias)\n","\n","# Hàm tính độ mới\n","def compute_novelty(new_behavior, behavior_archive, k=5, device = device):\n","    if not behavior_archive:\n","        return 0\n","    behavior_archive = torch.tensor(np.array(behavior_archive), device=device, dtype=torch.float32).view(len(behavior_archive), -1)\n","    new_behavior = torch.tensor(new_behavior, device=device, dtype=torch.float32).view(1, -1)\n","    # L2 norm (Euclidian distance)\n","    distances = torch.norm(behavior_archive - new_behavior, dim=1)\n","    distances = distances[distances > 0]\n","    if len(distances) > 0:\n","        sorted_distances, _ = torch.topk(distances, k=min(k, len(distances)), largest=False)\n","        return torch.mean(sorted_distances).item()\n","    else:\n","        return 0.1  # smoothed\n","# Sửa hàm tính đặc trưng hành vi\n","\n","def behavior_characterization(policy, env, device, seed=1, max_steps=200):\n","    policy.eval()\n","    behaviors = []\n","    step = 0\n","    state = env.reset(seed=seed)\n","    done = False\n","\n","    while not done:\n","        if isinstance(state, tuple):\n","            state = state[0]\n","        if isinstance(state, gym.wrappers.frame_stack.LazyFrames):\n","            state = np.array(state)\n","        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            action_probs = policy(state_tensor)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, _, done, _, _ = env.step(action)\n","        behaviors.append(state_tensor)\n","        # behaviors.append(state)\n","        state = next_state\n","        step += 1\n","        if step == max_steps:\n","            break\n","\n","    behaviors_tensor = torch.stack(behaviors, dim=0)  # (steps, *)\n","    return torch.mean(behaviors_tensor, dim=0).cpu().numpy()\n","    # return np.mean(behaviors, axis=0)\n","\n","def evaluate_policy(policy, env, device, seed=1, max_steps=200):\n","    total_rewards = 0\n","    state = env.reset(seed=seed)\n","    done = False\n","    trajectory = []\n","    step = 0\n","\n","    while not done:\n","        if isinstance(state, tuple):\n","            state = state[0]\n","        if isinstance(state, gym.wrappers.frame_stack.LazyFrames):\n","            state = np.array(state)\n","        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            action_probs = policy(state_tensor)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, done, _, _ = env.step(action)\n","        trajectory.append(state_tensor)\n","        # trajectory.append(state)\n","        total_rewards += reward\n","        state = next_state\n","        step += 1\n","        if step == max_steps:\n","            break\n","\n","    trajectory_tensor = torch.stack(trajectory, dim=0)  # (steps, *)\n","    return torch.mean(trajectory_tensor, dim=0).cpu().numpy(), total_rewards\n","    # return np.mean(trajectory, axis=0), total_rewards\n","\n","def play(policy, env_id, device, seed, max_steps, video_dir):\n","    env = make_env(env_id, capture_video = True, seed = seed)\n","    num_actions = env.action_space.n\n","    action_net = NeuralNetwork(num_actions, device).to(device)\n","    action_net.load_state_dict(policy.state_dict())\n","    action_net.eval()\n","    total_rewards = 0\n","    step = 0\n","    state, info = env.reset()\n","    done = False\n","    while not done:\n","        if isinstance(state, tuple):\n","                state = state[0]\n","        if isinstance(state, gym.wrappers.frame_stack.LazyFrames):\n","            state = np.array(state)\n","        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            action_probs = action_net(state_tensor)\n","            action = torch.argmax(action_probs, dim=1).item()\n","        next_state, reward, done, _, _ = env.step(action)\n","        step += 1\n","        total_rewards += reward\n","        state = next_state\n","        if done or step == max_steps:\n","            break\n","    env.close()\n","    return step, total_rewards\n","\n","# Hàm hiển thị video chơi game\n","def display_video(video_path):\n","    return Video(video_path, embed=True, width=720, height=480)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:48:31.152306Z","iopub.execute_input":"2024-12-25T11:48:31.153004Z","iopub.status.idle":"2024-12-25T11:48:31.171303Z","shell.execute_reply.started":"2024-12-25T11:48:31.152972Z","shell.execute_reply":"2024-12-25T11:48:31.170320Z"},"id":"0B_DyRiVNNAd"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Parameters & Hyperparameters"],"metadata":{"id":"6Wmyss0iNNAd"}},{"cell_type":"code","source":["# Hyperparameters\n","learning_rate = 1e-4\n","gamma = 0.99\n","seed = 21521992\n","pop_size = 3\n","update_frequency = 100\n","log_frequency = 50\n","sigma = 0.1\n","alpha = 0.001\n","n_workers = 3\n","max_iterations = 5000\n","max_steps = 2000\n","\n","\n","# Parameters\n","env_id = 'BreakoutNoFrameskip-v4'\n","capture_video = True\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:48:34.871271Z","iopub.execute_input":"2024-12-25T11:48:34.871613Z","iopub.status.idle":"2024-12-25T11:48:34.877277Z","shell.execute_reply.started":"2024-12-25T11:48:34.871569Z","shell.execute_reply":"2024-12-25T11:48:34.876412Z"},"id":"0Ab0lKKVNNAd"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# NS-ES Training"],"metadata":{"id":"lA5p0YNNNNAe"}},{"cell_type":"code","source":["all_best_states = []\n","all_best_rewards = []\n","all_novelties = []\n","training_times = []\n","\n","# Tạo thư mục lưu trữ kết quả\n","\n","save_dir = f'/kaggle/working/runs/NS-ES/{env_id}__{seed}'\n","video_dir = f'/kaggle/working/videos/NS-ES/{env_id}__{seed}'\n","os.makedirs(save_dir,exist_ok=True)\n","os.makedirs(video_dir,exist_ok=True)\n","video_result_log = video_dir + '/video_log.txt'\n","\n","# Khởi tạo\n","env = make_env(env_id, capture_video = False)\n","num_actions = env.action_space.n\n","\n","population = [NeuralNetwork(num_actions, device).apply(initialize_weights).to(device) for _ in range(pop_size)]\n","behavior_archive = deque(maxlen=500)\n","rewards = []\n","\n","# Tính toán BC và thêm vào archive\n","for policy in population:\n","    behavior_archive.append(behavior_characterization(policy, env, device, seed, max_steps))\n","\n","for iter in tqdm(range(max_iterations)):\n","# for iter in tqdm(range(20)):\n","    # Tính điểm thưởng của quần thể hiện tại\n","    pop_rewards = []\n","    pop_behaviors = []\n","    pop_novelties = []\n","\n","    start_time = time.time()\n","\n","    # Tính toán behavior + novelty (để sample) và điểm thưởng của quần thể\n","    for policy in population:\n","        behavior, reward = evaluate_policy(policy, env, device, seed, max_steps)\n","        pop_rewards.append(reward)\n","        pop_behaviors.append(behavior)\n","        pop_novelties.append(compute_novelty(behavior, behavior_archive))\n","\n","    best_index = np.argmax(pop_rewards)\n","    all_best_rewards.append(max(pop_rewards))\n","    all_best_states.append(population[best_index].state_dict())\n","\n","    # Sample\n","    probs = [novelty / sum(pop_novelties) for novelty in pop_novelties]\n","    selected_index = np.random.choice(range(pop_size), p=probs)\n","    selected_policy = population[selected_index]\n","\n","    # Thêm nhiễu\n","    policies = [selected_policy for _ in range(n_workers)]\n","    new_novelties = []\n","    for i in range(n_workers):\n","        for param in policies[i].parameters():\n","            if param.requires_grad:\n","                noise = torch.normal(0, sigma, size=param.size(), device=param.device)\n","                param.data.add_(noise)\n","        bc = behavior_characterization(policies[i], env, device, seed, max_steps)\n","        new_novelties.append(compute_novelty(bc, behavior_archive))\n","\n","    # Cập nhật chính sách\n","    for param in population[selected_index].parameters():\n","        if param.requires_grad:\n","            noise = torch.normal(0, sigma, size=param.size(), device=param.device)\n","            novelty_mean = torch.mean(torch.tensor(new_novelties, device=param.device))\n","\n","            # scale dựa theo phân phối\n","            # novelty_std = torch.std(torch.tensor(new_novelties, device=param.device))\n","            # if novelty_std > 0:\n","            #     norm_novelty = (torch.tensor(new_novelties, device=param.device) - novelty_mean) / novelty_std\n","            # else:\n","            #     norm_novelty = torch.zeros_like(torch.tensor(new_novelties, device=param.device))\n","\n","            # scale bằng hàm kích hoạt phi tuyến\n","            norm_novelty = torch.tanh(torch.tensor(new_novelties, device=param.device))       # hoặc đổi thành sigmoid\n","            scale_value =  torch.mean(norm_novelty)\n","            update_value = alpha * (1 / sigma) * scale_value * noise\n","\n","            # update_value = alpha * novelty_mean * (1 / sigma) * noise\n","            param.data.add_(update_value)\n","\n","    # Thêm vào archive\n","    bc = behavior_characterization(population[selected_index], env, device, seed, max_steps)\n","    all_novelties.append(compute_novelty(bc, behavior_archive))\n","    behavior_archive.append(bc)\n","\n","    training_times.append(time.time() - start_time)\n","\n","    if (iter+1) % update_frequency == 0:\n","    # if (iter+1) % 2 == 0:\n","        best_new_novelty = np.max(all_novelties[-5:]) if iter >= 5 else np.max(all_novelties)\n","        best_new_reward = np.max(all_best_rewards[-5:]) if iter >= 5 else np.max(all_best_rewards)\n","        print(f\"Iteration {iter+1}/{max_iterations}, Best new novelty: {best_new_novelty:.2f}, Best new reward: {best_new_reward:.2f}, Total executing time: {sum(training_times):.2f} seconds\")\n","\n","    if (iter+1) % log_frequency == 0:\n","    # if (iter+1) % 2 == 0:\n","        # Lưu trạng thái quần thể\n","        np.savez_compressed(os.path.join(save_dir,f'results.npz'),\n","                            rewards = all_best_rewards, training_times = training_times)\n","        torch.save(all_best_states, os.path.join(save_dir,f\"Iteration_{iter+1}.pth\"))\n","\n","        # Chơi game và lưu video\n","        best_policy_state = all_best_states[-100:][np.argsort(all_best_rewards[-100:])[-1]]\n","        best_policy = NeuralNetwork(num_actions, device).to(device)\n","        best_policy.load_state_dict(best_policy_state)\n","        best_policy.eval()\n","        total_step, total_reward = play(best_policy, env_id, device, seed, max_steps, video_dir)"],"metadata":{"trusted":true,"id":"kD4eRMo0NNAe"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# NSR-ES Training"],"metadata":{"id":"D3k-L_7bNNAe"}},{"cell_type":"code","source":["# hyperparameters\n","novelty_weight = 0.4\n","reward_weight = 1 - novelty_weight"],"metadata":{"id":"2lwjJV3pcOPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_best_states = []\n","all_best_rewards = []\n","all_novelties = []\n","training_times = []\n","\n","# Tạo thư mục lưu trữ kết quả\n","\n","save_dir = f'/kaggle/working/runs/NSR-ES/{env_id}__{seed}'\n","video_dir = f'/kaggle/working/videos/NSR-ES/{env_id}__{seed}'\n","os.makedirs(save_dir,exist_ok=True)\n","os.makedirs(video_dir,exist_ok=True)\n","video_result_log = video_dir + '/video_log.txt'\n","\n","# Khởi tạo\n","env = make_env(env_id, capture_video = False)\n","num_actions = env.action_space.n\n","\n","population = [NeuralNetwork(num_actions, device).apply(initialize_weights).to(device) for _ in range(pop_size)]\n","behavior_archive = deque(maxlen=500)\n","rewards = []\n","\n","# Tính toán BC và thêm vào archive\n","for policy in population:\n","    behavior_archive.append(behavior_characterization(policy, env, device, seed, max_steps))\n","\n","for iter in tqdm(range(max_iterations)):\n","# for iter in tqdm(range(20)):\n","    # Tính điểm thưởng của quần thể hiện tại\n","    pop_rewards = []\n","    pop_behaviors = []\n","    pop_novelties = []\n","\n","    start_time = time.time()\n","\n","    # Tính toán behavior + novelty (để sample) và điểm thưởng của quần thể\n","    for policy in population:\n","        behavior, reward = evaluate_policy(policy, env, device, seed, max_steps)\n","        pop_rewards.append(reward)\n","        pop_behaviors.append(behavior)\n","        pop_novelties.append(compute_novelty(behavior, behavior_archive))\n","\n","    best_index = np.argmax(pop_rewards)\n","    all_best_rewards.append(max(pop_rewards))\n","    all_best_states.append(population[best_index].state_dict())\n","\n","    # Sample\n","    probs = [novelty / sum(pop_novelties) for novelty in pop_novelties]\n","    selected_index = np.random.choice(range(pop_size), p=probs)\n","    selected_policy = population[selected_index]\n","\n","    # Thêm nhiễu\n","    policies = [selected_policy for _ in range(n_workers)]\n","    new_novelties = []\n","    new_rewards = []\n","\n","    for i in range(n_workers):\n","        for param in policies[i].parameters():\n","            if param.requires_grad:\n","                noise = torch.normal(0, sigma, size=param.size(), device=param.device)\n","                param.data.add_(noise)\n","        bc, score = evaluate_policy(policies[i], env, device, seed, max_steps)\n","        new_novelties.append(compute_novelty(bc, behavior_archive))\n","        new_rewards.append(score)\n","\n","    # Cập nhật chính sách\n","    for param in population[selected_index].parameters():\n","        if param.requires_grad:\n","            noise = torch.normal(0, sigma, size=param.size(), device=param.device)\n","            novelty_mean = torch.mean(torch.tensor(new_novelties, device=param.device))\n","            reward_mean = torch.mean(torch.tensor(new_rewards, device = param.device))\n","\n","            norm_novelty = torch.tanh(novelty_mean)       # hoặc đổi thành sigmoid\n","            norm_reward = torch.tanh(reward_mean)\n","            update_value = alpha * (1 / sigma) * (novelty_weight * norm_novelty + reward_weight * norm_reward) * noise\n","\n","            # update_value = alpha * novelty_mean * (1 / sigma) * noise\n","            param.data.add_(update_value)\n","\n","    # Thêm vào archive\n","    bc = behavior_characterization(population[selected_index], env, device, seed, max_steps)\n","    all_novelties.append(compute_novelty(bc, behavior_archive))\n","    behavior_archive.append(bc)\n","\n","    training_times.append(time.time() - start_time)\n","\n","    if (iter+1) % update_frequency == 0:\n","    # if (iter+1) % 2 == 0:\n","        best_new_novelty = np.max(all_novelties[-5:]) if iter >= 5 else np.max(all_novelties)\n","        best_new_reward = np.max(all_best_rewards[-5:]) if iter >= 5 else np.max(all_best_rewards)\n","        print(f\"Iteration {iter+1}/{max_iterations}, Best new novelty: {best_new_novelty:.2f}, Best new reward: {best_new_reward:.2f}, Total executing time: {sum(training_times):.2f} seconds\")\n","\n","    if (iter+1) % log_frequency == 0:\n","    # if (iter+1) % 2 == 0:\n","        # Lưu trạng thái quần thể\n","        np.savez_compressed(os.path.join(save_dir,f'results.npz'),\n","                            rewards = all_best_rewards, training_times = training_times)\n","        torch.save(all_best_states, os.path.join(save_dir,f\"Iteration_{iter+1}.pth\"))\n","\n","        # Chơi game và lưu video\n","        best_policy_state = all_best_states[-100:][np.argsort(all_best_rewards[-100:])[-1]]\n","        best_policy = NeuralNetwork(num_actions, device).to(device)\n","        best_policy.load_state_dict(best_policy_state)\n","        best_policy.eval()\n","        total_step, total_reward = play(best_policy, env_id, device, seed, max_steps, video_dir)\n"],"metadata":{"trusted":true,"id":"DmxCtW9QNNAe"},"outputs":[],"execution_count":null}]}