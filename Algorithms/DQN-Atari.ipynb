{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries & Environment","metadata":{}},{"cell_type":"code","source":"!pip install gym\n!pip install gymnasium\n!pip install ale-py==0.8.1\n!pip install \"autorom[accept-rom-license]==0.4.2\"\n!pip install pygame==2.1.0\n!pip install imageio==2.36.1\n!pip install \"imageio-ffmpeg==0.5.1\"\n!pip install moviepy==2.1.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:11:34.126312Z","iopub.execute_input":"2024-12-24T17:11:34.126589Z","iopub.status.idle":"2024-12-24T17:12:58.790721Z","shell.execute_reply.started":"2024-12-24T17:11:34.126544Z","shell.execute_reply":"2024-12-24T17:12:58.789612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nfrom collections import deque\nimport time\nimport os\nfrom moviepy import ImageSequenceClip\nimport cv2\nfrom gym.wrappers import AtariPreprocessing, FrameStack\nfrom gym.wrappers import RecordVideo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:12:58.79234Z","iopub.execute_input":"2024-12-24T17:12:58.792683Z","iopub.status.idle":"2024-12-24T17:13:06.51223Z","shell.execute_reply.started":"2024-12-24T17:12:58.792653Z","shell.execute_reply":"2024-12-24T17:13:06.51158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self, action_space, device):\n        super(NeuralNetwork, self).__init__()\n\n        self.device = device\n        \n        self.net = nn.Sequential(\n            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3136, 512),\n            nn.ReLU(),\n            nn.Linear(512, action_space)\n        )\n\n    def forward(self, x):\n        return self.net(x / 255.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:13:06.514001Z","iopub.execute_input":"2024-12-24T17:13:06.514341Z","iopub.status.idle":"2024-12-24T17:13:06.519932Z","shell.execute_reply.started":"2024-12-24T17:13:06.514307Z","shell.execute_reply":"2024-12-24T17:13:06.519103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ReplayBuffer:\n    def __init__(self, size):\n        self.buffer = deque(maxlen=size)\n\n    def add(self, obs, next_obs, action, reward, done):\n        self.buffer.append((obs, next_obs, action, reward, done))\n\n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        obs, next_obs, actions, rewards, dones = zip(*batch)\n        return (\n            np.stack(obs),\n            np.stack(next_obs),\n            np.array(actions),\n            np.array(rewards),\n            np.array(dones)\n        )\n\n    def size(self):\n        return len(self.buffer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:13:06.520809Z","iopub.execute_input":"2024-12-24T17:13:06.521123Z","iopub.status.idle":"2024-12-24T17:13:06.538724Z","shell.execute_reply.started":"2024-12-24T17:13:06.521083Z","shell.execute_reply":"2024-12-24T17:13:06.53809Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define used functions","metadata":{}},{"cell_type":"code","source":"def make_env(env_id, record = False):\n    if capture_video:\n        env = gym.make(env_id, render_mode=\"rgb_array\")\n        env = gym.wrappers.RecordVideo(env, \"/kaggle/working/videos\")\n    else:\n        env = gym.make(env_id)\n    env = gym.wrappers.RecordEpisodeStatistics(env)\n    env = gym.wrappers.ResizeObservation(env, (84, 84))\n    env = gym.wrappers.GrayScaleObservation(env)\n    env = gym.wrappers.FrameStack(env, 4)\n    env.action_space.seed(seed)\n    return env\n\ndef linear_schedule(start_e, end_e, duration, t):\n    slope = (end_e - start_e) / duration\n    return max(slope * t + start_e, end_e)\n\ndef preprocess_frame(frame):\n    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  \n    resized_frame = cv2.resize(gray_frame, (84, 84), interpolation=cv2.INTER_AREA)  \n    return resized_frame / 255.0\n\ndef play(env_id, q_network, device, seed=1, max_steps=5000, capture_video=False, video_output_dir='/kaggle/working/videos'):\n    if capture_video:\n        env = make_env(env_id, record = capture_video)\n        env = RecordVideo(env, video_folder=video_output_dir, episode_trigger=lambda episode: True)  # Ghi tất cả các episode\n        env.seed(seed)\n    else:\n        env = make_env(env_id, record=capture_video)\n    \n    action_network = NeuralNetwork(env.action_space.n, device=device).to(device)\n    action_network.load_state_dict(q_network.state_dict())\n\n    state, _ = env.reset(seed=seed)\n    total_rewards = 0\n    total_steps = 0\n    done = False\n    with torch.no_grad():\n        while not done:\n            if type(state) == tuple:\n                state = state[0]\n            q_value = action_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))\n            action = torch.argmax(q_value, dim=1).item()\n            next_state, reward, done, truncated, info = env.step(action)\n            total_rewards += reward\n            total_steps += 1\n            state = next_state\n            \n            if total_steps == max_steps:\n                break\n    env.close()\n\n    \n    return total_rewards, total_steps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:13:06.539478Z","iopub.execute_input":"2024-12-24T17:13:06.539721Z","iopub.status.idle":"2024-12-24T17:13:06.552494Z","shell.execute_reply.started":"2024-12-24T17:13:06.539699Z","shell.execute_reply":"2024-12-24T17:13:06.551773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Parameters & Hyperparameters","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nlearning_rate = 1e-4\ngamma = 0.99\nbatch_size = 32\ntarget_network_frequency = 1000\ntrain_frequency = 4\nlog_frequency = 10000\nexploration_fraction = 0.1\nstart_e = 1.0\nend_e = 0.01\nlearning_starts = 10000\ntotal_timesteps = 10000000\ntau = 1.0\nreplay_buffer_size = 100000\nseed = 21521992\n\n# Parameters\nenv_id = 'BreakoutNoFrameskip-v4'\ncapture_video = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nenv = make_env(env_id, capture_video)\nnum_actions = env.action_space.n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:17:03.989437Z","iopub.execute_input":"2024-12-24T17:17:03.989782Z","iopub.status.idle":"2024-12-24T17:17:04.172286Z","shell.execute_reply.started":"2024-12-24T17:17:03.989755Z","shell.execute_reply":"2024-12-24T17:17:04.171394Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"q_network = NeuralNetwork(num_actions, device = device).to(device)\ntarget_network = NeuralNetwork(num_actions, device = device).to(device)\ntarget_network.load_state_dict(q_network.state_dict())\ntarget_network.eval()\noptimizer = optim.Adam(q_network.parameters(), lr=learning_rate)\nreplay_buffer = ReplayBuffer(replay_buffer_size)\nsave_dir = f'/kaggle/working/runs/{env_id}__{seed}'\nvideo_dir = f'/kaggle/working/videos/{env_id}__{seed}'\nos.makedirs(save_dir,exist_ok=True)\nos.makedirs(video_dir,exist_ok=True)\n\n\nobs, _ = env.reset()\nobs = np.array(obs)\nreward_buffer = deque(maxlen=100)\nreward_per_episode = 0.0\nall_rewards = []\ntraining_times = []\n\n\nfor global_step in range(total_timesteps):\n    start_time = time.time()\n    epsilon = linear_schedule(start_e, end_e, exploration_fraction * total_timesteps, global_step)\n    if random.random() < epsilon:\n        action = env.action_space.sample()\n    else:\n        with torch.no_grad():\n            if type(obs) == tuple:\n                obs = obs[0]\n            q_values = q_network(torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device))\n            action = torch.argmax(q_values, dim=1).item()\n            \n    next_obs, reward, done, truncated, info = env.step(action)\n    reward_per_episode += reward\n    if type(next_obs) == tuple:\n        next_obs = next_obs[0]\n    replay_buffer.add(obs, next_obs, action, reward, done)\n    obs = next_obs\n    \n    if done:\n        obs, _ = env.reset(seed=seed)\n        reward_buffer.append(reward_per_episode)\n        all_rewards.append((global_step, reward_per_episode))\n        reward_per_episode = 0.0\n\n    #  Q-net\n    if (global_step + 1) % train_frequency == 0 and replay_buffer.size() > batch_size:\n        obs_batch, next_obs_batch, actions_batch, rewards_batch, dones_batch = replay_buffer.sample(batch_size)\n        obs_batch = torch.tensor(obs_batch, dtype=torch.float32).to(device)\n        next_obs_batch = torch.tensor(next_obs_batch, dtype=torch.float32).to(device)\n        actions_batch = torch.tensor(actions_batch, dtype=torch.long).unsqueeze(1).to(device)\n        rewards_batch = torch.tensor(rewards_batch, dtype=torch.float32).to(device)\n        dones_batch = torch.tensor(dones_batch, dtype=torch.float32).to(device)\n\n        # Compute targets using the formulation sample = r + gamma * max q(s',a')\n        with torch.no_grad():\n            target_max = target_network(next_obs_batch).max(dim=1)[0]\n            td_target = rewards_batch + gamma * target_max * (1 - dones_batch)\n            \n        # Compute loss\n        q_values = q_network(obs_batch).gather(1, actions_batch).squeeze()\n        loss = nn.functional.mse_loss(q_values, td_target)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    end_time = time.time()\n    training_times.append(end_time - start_time)\n    \n    if (global_step + 1) % target_network_frequency == 0:\n        target_network.load_state_dict(q_network.state_dict())\n\n    if (global_step + 1) % 1000 == 0:\n        q_net_path = f\"{save_dir}/Step__{global_step + 1}.pth\"\n        video_record_path = f\"{video_dir}/Step__{global_step + 1}.mp4\"\n        torch.save(q_network.state_dict(), q_net_path)\n        np.savez_compressed(save_dir + '/results.npz', all_rewards=all_rewards, training_times = training_times)\n \n\n    if (global_step + 1) % 10000 == 0:\n        average_reward = np.mean(reward_buffer)\n        print(f'Episode: {len(all_rewards)} Step: {global_step+1}/{total_timesteps} Average reward: {average_reward:.2f} Total training time: {sum(training_times):.2f} seconds')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T17:17:07.692039Z","iopub.execute_input":"2024-12-24T17:17:07.692379Z"}},"outputs":[],"execution_count":null}]}