{"cells":[{"cell_type":"markdown","metadata":{"id":"UCf5zmdHNOel"},"source":["# Libraries & Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-25T11:06:28.555672Z","iopub.status.busy":"2024-12-25T11:06:28.555265Z","iopub.status.idle":"2024-12-25T11:07:50.151104Z","shell.execute_reply":"2024-12-25T11:07:50.149776Z","shell.execute_reply.started":"2024-12-25T11:06:28.555624Z"},"trusted":true,"id":"FjCY0UATNOem","outputId":"201cbda9-18ee-450f-93c8-8d74e5d62933"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gym in /opt/conda/lib/python3.10/site-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym) (3.1.0)\n","Requirement already satisfied: gym_notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym) (0.0.8)\n","Requirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (0.29.0)\n","Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n","Collecting ale-py==0.8.1\n","  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ale-py==0.8.1) (1.26.4)\n","Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py==0.8.1) (6.4.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py==0.8.1) (4.12.2)\n","Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: ale-py\n","Successfully installed ale-py-0.8.1\n","Collecting autorom==0.4.2 (from autorom[accept-rom-license]==0.4.2)\n","  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (8.1.7)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (2.32.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (4.66.4)\n","Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]==0.4.2)\n","  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom==0.4.2->autorom[accept-rom-license]==0.4.2) (2024.6.2)\n","Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446667 sha256=500bc011dd88068e2fd482abe72807c7f02fa15e35a398aa018b2852c09d11bb\n","  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: AutoROM.accept-rom-license, autorom\n","Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n","Collecting pygame==2.1.0\n","  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n","Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-2.1.0\n","Collecting imageio==2.36.1\n","  Downloading imageio-2.36.1-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from imageio==2.36.1) (1.26.4)\n","Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio==2.36.1) (10.3.0)\n","Downloading imageio-2.36.1-py3-none-any.whl (315 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.4/315.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: imageio\n","  Attempting uninstall: imageio\n","    Found existing installation: imageio 2.34.1\n","    Uninstalling imageio-2.34.1:\n","      Successfully uninstalled imageio-2.34.1\n","Successfully installed imageio-2.36.1\n","Collecting imageio-ffmpeg==0.5.1\n","  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio-ffmpeg==0.5.1) (70.0.0)\n","Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: imageio-ffmpeg\n","Successfully installed imageio-ffmpeg-0.5.1\n","Collecting moviepy==2.1.1\n","  Downloading moviepy-2.1.1-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: decorator<6.0,>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (5.1.1)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (2.36.1)\n","Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (0.5.1)\n","Requirement already satisfied: numpy>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (1.26.4)\n","Collecting proglog<=1.0.0 (from moviepy==2.1.1)\n","  Downloading proglog-0.1.10-py3-none-any.whl.metadata (639 bytes)\n","Requirement already satisfied: python-dotenv>=0.10 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (1.0.1)\n","Requirement already satisfied: pillow<11.0,>=9.2.0 in /opt/conda/lib/python3.10/site-packages (from moviepy==2.1.1) (10.3.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio_ffmpeg>=0.2.0->moviepy==2.1.1) (70.0.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from proglog<=1.0.0->moviepy==2.1.1) (4.66.4)\n","Downloading moviepy-2.1.1-py3-none-any.whl (123 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n","Installing collected packages: proglog, moviepy\n","Successfully installed moviepy-2.1.1 proglog-0.1.10\n"]}],"source":["!pip install gym\n","!pip install gymnasium\n","!pip install ale-py==0.8.1\n","!pip install \"autorom[accept-rom-license]==0.4.2\"\n","!pip install pygame==2.1.0\n","!pip install imageio==2.36.1\n","!pip install \"imageio-ffmpeg==0.5.1\"\n","!pip install moviepy==2.1.1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-25T11:07:50.153268Z","iopub.status.busy":"2024-12-25T11:07:50.152974Z","iopub.status.idle":"2024-12-25T11:07:53.179790Z","shell.execute_reply":"2024-12-25T11:07:53.178940Z","shell.execute_reply.started":"2024-12-25T11:07:50.153237Z"},"trusted":true,"id":"p5_eRBqZNOeo"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions.categorical import Categorical\n","import random\n","from collections import deque\n","import time\n","import os\n","from moviepy import ImageSequenceClip\n","import cv2\n","from gym.wrappers import AtariPreprocessing, FrameStack\n","from gym.wrappers import RecordVideo\n","import numpy as np\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"hrKJ5zpfNOep"},"source":["# Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-25T11:48:29.501122Z","iopub.status.busy":"2024-12-25T11:48:29.500336Z","iopub.status.idle":"2024-12-25T11:48:29.506787Z","shell.execute_reply":"2024-12-25T11:48:29.505881Z","shell.execute_reply.started":"2024-12-25T11:48:29.501088Z"},"trusted":true,"id":"OLK28KP-NOep"},"outputs":[],"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self, action_space, device):\n","        super(NeuralNetwork, self).__init__()\n","\n","        self.device = device\n","\n","        self.net = nn.Sequential(\n","            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3136, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, action_space)\n","        )\n","\n","        # self.net = nn.Sequential(\n","        #     nn.Conv2d(4, 16, kernel_size=4, stride=2),\n","        #     nn.ReLU(),\n","        #     nn.Conv2d(16, 32, kernel_size=3, stride=2),\n","        #     nn.ReLU(),\n","        #     nn.Flatten(),\n","        #     nn.Linear(32 * 9 * 9, 128),\n","        #     nn.ReLU(),\n","        #     nn.Linear(128, action_space)\n","        # )\n","\n","    def forward(self, x):\n","        return self.net(x / 255.0)"]},{"cell_type":"markdown","metadata":{"id":"HSqrgCZQNOep"},"source":["# Define used functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-25T11:07:53.190117Z","iopub.status.busy":"2024-12-25T11:07:53.189760Z","iopub.status.idle":"2024-12-25T11:07:53.423993Z","shell.execute_reply":"2024-12-25T11:07:53.423151Z","shell.execute_reply.started":"2024-12-25T11:07:53.190080Z"},"trusted":true,"id":"EcoHHyM3NOeq","outputId":"ec8ca79e-5052-4234-91cd-efcfdacb28e3"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-25T11:48:31.153004Z","iopub.status.busy":"2024-12-25T11:48:31.152306Z","iopub.status.idle":"2024-12-25T11:48:31.171303Z","shell.execute_reply":"2024-12-25T11:48:31.170320Z","shell.execute_reply.started":"2024-12-25T11:48:31.152972Z"},"trusted":true,"id":"xz55r5wrNOeq"},"outputs":[],"source":["def make_env(env_id, capture_video = False, seed = 1):\n","    if capture_video:\n","        env = gym.make(env_id, render_mode=\"rgb_array\")\n","        env = gym.wrappers.RecordVideo(env, \"/kaggle/working/videos\")\n","    else:\n","        env = gym.make(env_id)\n","    env = gym.wrappers.RecordEpisodeStatistics(env)\n","    env = gym.wrappers.ResizeObservation(env, (84, 84))\n","    env = gym.wrappers.GrayScaleObservation(env)\n","    env = gym.wrappers.FrameStack(env, 4)\n","    env.action_space.seed(seed)\n","    return env\n","\n","def linear_schedule(start_e, end_e, duration, t):\n","    slope = (end_e - start_e) / duration\n","    return max(slope * t + start_e, end_e)\n","\n","# Thay đổi hàm tính độ mới. Ở đây mỗi khi cập nhật archive, mỗi hành vi đều được coi là có sự đóng góp vào tính mới của archive\n","def compute_novelty(behavior_archive, k=5):\n","    novelty_scores = []\n","    for i in range(len(behavior_archive)):\n","        distances = [np.linalg.norm(behavior_archive[i] - b) for j, b in enumerate(behavior_archive) if i != j]\n","        novelty_scores.append(np.mean(sorted(distances)[:k]))\n","    return novelty_scores\n","\n","def behavior_characterization(policy, env, device, seed=1, max_steps=200):\n","    policy.eval()\n","    behaviors = []\n","    step = 0\n","    state = env.reset(seed=seed)\n","    done = False\n","\n","    while not done:\n","        if isinstance(state, tuple):\n","            state = state[0]\n","        if isinstance(state, gym.wrappers.frame_stack.LazyFrames):\n","            state = np.array(state)\n","        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            action_probs = policy(state_tensor)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, _, done, _, _ = env.step(action)\n","        behaviors.append(state_tensor)\n","        # behaviors.append(state)\n","        state = next_state\n","        step += 1\n","        if step == max_steps:\n","            break\n","\n","    behaviors_tensor = torch.stack(behaviors, dim=0)  # (steps, *)\n","    return torch.mean(behaviors_tensor, dim=0).cpu().numpy()\n","    # return np.mean(behaviors, axis=0)\n","\n","def evaluate_policy(policy, env, device, seed=1, max_steps=200):\n","    total_rewards = 0\n","    state = env.reset(seed=seed)\n","    done = False\n","    trajectory = []\n","    step = 0\n","\n","    while not done:\n","        if isinstance(state, tuple):\n","            state = state[0]\n","        if isinstance(state, gym.wrappers.frame_stack.LazyFrames):\n","            state = np.array(state)\n","        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","\n","        with torch.no_grad():\n","            action_probs = policy(state_tensor)\n","            action = torch.argmax(action_probs, dim=1).item()\n","\n","        next_state, reward, done, _, _ = env.step(action)\n","        trajectory.append(state_tensor)\n","        # trajectory.append(state)\n","        total_rewards += reward\n","        state = next_state\n","        step += 1\n","        if step == max_steps:\n","            break\n","\n","    trajectory_tensor = torch.stack(trajectory, dim=0)  # (steps, *)\n","    return torch.mean(trajectory_tensor, dim=0).cpu().numpy(), total_rewards\n","    # return np.mean(trajectory, axis=0), total_rewards\n","\n","def play(policy, env_id, device, seed, max_steps, video_dir):\n","    env = make_env(env_id, capture_video = True, seed = seed)\n","    num_actions = env.action_space.n\n","    action_net = NeuralNetwork(num_actions, device).to(device)\n","    action_net.load_state_dict(policy.state_dict())\n","    action_net.eval()\n","    total_rewards = 0\n","    step = 0\n","    state, info = env.reset()\n","    done = False\n","    while not done:\n","        if isinstance(state, tuple):\n","                state = state[0]\n","        if isinstance(state, gym.wrappers.frame_stack.LazyFrames):\n","            state = np.array(state)\n","        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n","        with torch.no_grad():\n","            action_probs = action_net(state_tensor)\n","            action = torch.argmax(action_probs, dim=1).item()\n","        next_state, reward, done, _, _ = env.step(action)\n","        step += 1\n","        total_rewards += reward\n","        state = next_state\n","        if done or step == max_steps:\n","            break\n","    env.close()\n","    return step, total_rewards\n","\n","# Hàm hiển thị video chơi game\n","def display_video(video_path):\n","    return Video(video_path, embed=True, width=720, height=480)"]},{"cell_type":"markdown","metadata":{"id":"fUm5iy2bNOer"},"source":["# Parameters & Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-25T11:48:34.871613Z","iopub.status.busy":"2024-12-25T11:48:34.871271Z","iopub.status.idle":"2024-12-25T11:48:34.877277Z","shell.execute_reply":"2024-12-25T11:48:34.876412Z","shell.execute_reply.started":"2024-12-25T11:48:34.871569Z"},"trusted":true,"id":"3IDf5HwXNOes"},"outputs":[],"source":["# Hyperparameters\n","gamma = 0.99\n","seed = 21521992\n","pop_size = 20     # Kích thước quần thể\n","max_gens = 200    # Số lượng thế hệ\n","num_episodes_per_eval = 1\n","distance_count = 5    # Tham số k (kNN) khi tính toán novelty\n","update_frequency = 10\n","max_steps = 2000  # Giới hạn số bước tối đa cho mỗi tập\n","sigma = 0.1\n","alpha = 0.001\n","update_frequency = 10\n","log_frequency = 10\n","\n","\n","# Parameters\n","env_id = 'BreakoutNoFrameskip-v4'\n","capture_video = True"]},{"cell_type":"markdown","metadata":{"id":"BERHcrYnNOes"},"source":["# NS-ES Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"Cu_6EWBRNOes"},"outputs":[],"source":["save_dir = f'/kaggle/working/runs/NS-ESc/{env_id}__{seed}'\n","video_dir = f'/kaggle/working/videos/NS-ESc/{env_id}__{seed}'\n","os.makedirs(save_dir, exist_ok=True)\n","os.makedirs(video_dir, exist_ok=True)\n","video_result_log = video_dir + '/video_log.txt'\n","\n","# Khởi tạo\n","env = make_env(env_id, capture_video=False)\n","num_actions = env.action_space.n\n","\n","population = [NeuralNetwork(num_actions, device).to(device) for _ in range(pop_size)]\n","behavior_archive = deque(maxlen=500)\n","training_times = []\n","all_best_rewards = []\n","\n","# Tính toán BC và thêm vào archive\n","for policy in population:\n","    behavior_archive.append(behavior_characterization(policy, env, device, seed, max_steps))\n","\n","best_policy_state = None\n","best_reward = float('-inf')\n","\n","for gen in tqdm(range(max_gens)):\n","    pop_behaviors = []\n","    pop_rewards = []\n","\n","    start_time = time.time()\n","\n","    # Đánh giá quần thể\n","    for policy in population:\n","        behavior, reward = evaluate_policy(env, policy, num_episodes_per_eval, max_steps, seed)\n","        pop_behaviors.append(behavior)\n","        pop_rewards.append(reward)\n","\n","    all_best_rewards.append(np.max(pop_rewards))\n","\n","    # Thêm các behavior vào archive\n","    behavior_archive.extend(pop_behaviors)\n","\n","    # Đánh giá độ mới và xếp hạng quần thể\n","    novelties = compute_novelty(behavior_archive, k=distance_count)\n","    pop_novelties = novelties[-pop_size:]\n","\n","    # Lựa chọn 10 cá thể tốt nhất để giữ lại\n","    sorted_indices = np.argsort(pop_novelties)\n","    top_indices = sorted_indices[-10:]\n","    remain_indices = sorted_indices[:-10]\n","    new_population = [population[i] for i in top_indices]\n","    best_policy_state = population[top_indices[0]].state_dict()    # cá thể tốt nhất\n","\n","    # Lấy 10 cá thể tốt nhất và 30 cá thể trong số 40 cá thể còn lại để tiến hành đột biến\n","    mutant_indices = []\n","    mutant_indices.extend(top_indices)\n","    while len(mutant_indices) < pop_size - 10:\n","        mutant_indices.append(np.random.choice(remain_indices, replace=False))\n","\n","    for idx in mutant_indices:\n","        parent = population[idx]\n","        child = NeuralNetwork(num_actions, device).to(device)\n","        for child_param, parent_param in zip(child.parameters(), parent.parameters()):\n","            noise = torch.normal(mean=0.0, std=sigma, size=parent_param.shape, device=parent_param.device)\n","            child_param.data = parent_param.data + noise  # Thêm nhiễu\n","        new_population.append(child)\n","\n","    population = new_population\n","    training_times.append(time.time() - start_time)\n","\n","    # Lưu trạng thái cá thể tốt nhất\n","    if (gen + 1) % log_frequency == 0:\n","        torch.save(best_policy_state, os.path.join(save_dir, f\"Generation_{gen + 1}.pt\"))\n","        np.savez_compressed(os.path.join(save_dir,f'results.npz'),\n","                            rewards = all_best_rewards, training_times = training_times)\n","\n","        # Chơi game và lưu video\n","        best_policy = NeuralNetwork(num_actions, device).to(device)\n","        best_policy.load_state_dict(best_policy_state)\n","        best_policy.eval()\n","        total_step, total_reward = play(best_policy, env_id, device, seed, max_steps, video_dir)"]},{"cell_type":"markdown","metadata":{"id":"jFsyo9aSNOes"},"source":["# NSR-ES Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbKJd6GtNOes"},"outputs":[],"source":["# Hyperparameters\n","gamma = 0.99\n","seed = 21521992\n","pop_size = 20     # Kích thước quần thể\n","max_gens = 200    # Số lượng thế hệ\n","num_episodes_per_eval = 1\n","distance_count = 5    # Tham số k (kNN) khi tính toán novelty\n","update_frequency = 10\n","max_steps = 2000  # Giới hạn số bước tối đa cho mỗi tập\n","sigma = 0.1\n","alpha = 0.001\n","update_frequency = 10\n","log_frequency = 10\n","novelty_weight = 0.5\n","reward_weight = 1 - novelty_weight\n","\n","# Parameters\n","env_id = 'BreakoutNoFrameskip-v4'\n","capture_video = True"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"WnAq0e6HNOet"},"outputs":[],"source":["save_dir = f'/kaggle/working/runs/NSR-ESc/{env_id}__{seed}'\n","video_dir = f'/kaggle/working/videos/NSR-ESc/{env_id}__{seed}'\n","os.makedirs(save_dir, exist_ok=True)\n","os.makedirs(video_dir, exist_ok=True)\n","video_result_log = video_dir + '/video_log.txt'\n","\n","# Khởi tạo\n","env = make_env(env_id, capture_video=False)\n","num_actions = env.action_space.n\n","\n","population = [NeuralNetwork(num_actions, device).to(device) for _ in range(pop_size)]\n","behavior_archive = deque(maxlen=500)\n","training_times = []\n","all_best_rewards = []\n","\n","# Tính toán BC và thêm vào archive\n","for policy in population:\n","    behavior_archive.append(behavior_characterization(policy, env, device, seed, max_steps))\n","\n","best_policy_state = None\n","best_reward = float('-inf')\n","\n","for gen in tqdm(range(max_gens)):\n","    pop_behaviors = []\n","    pop_rewards = []\n","\n","    start_time = time.time()\n","\n","    # Đánh giá quần thể\n","    for policy in population:\n","        behavior, reward = evaluate_policy(env, policy, num_episodes_per_eval, max_steps, seed)\n","        pop_behaviors.append(behavior)\n","        pop_rewards.append(reward)\n","\n","    # Thêm các behavior vào archive\n","    behavior_archive.extend(pop_behaviors)\n","\n","    all_best_rewards.append(np.max(pop_rewards))\n","\n","    # Đánh giá độ mới và xếp hạng quần thể\n","    novelties = compute_novelty(behavior_archive, k=distance_count)\n","    pop_novelties = novelties[-pop_size:]\n","    pop_scores = novelty_weight * np.array(pop_novelties) + reward_weight * np.array(pop_rewards)\n","\n","    # Lựa chọn 10 cá thể tốt nhất để giữ lại\n","    sorted_indices = np.argsort(pop_scores)\n","    top_indices = sorted_indices[-10:]\n","    remain_indices = sorted_indices[:-10]\n","    new_population = [population[i] for i in top_indices]\n","    best_policy_state = population[top_indices[0]].state_dict()    # cá thể tốt nhất\n","\n","    # Lấy 10 cá thể tốt nhất và 30 cá thể trong số 40 cá thể còn lại để tiến hành đột biến\n","    mutant_indices = []\n","    mutant_indices.extend(top_indices)\n","    while len(mutant_indices) < pop_size - 10:\n","        mutant_indices.append(np.random.choice(remain_indices, replace=False))\n","\n","    for idx in mutant_indices:\n","        parent = population[idx]\n","        child = NeuralNetwork(num_actions, device).to(device)\n","        for child_param, parent_param in zip(child.parameters(), parent.parameters()):\n","            noise = torch.normal(mean=0.0, std=sigma, size=parent_param.shape, device=parent_param.device)\n","            child_param.data = parent_param.data + noise  # Thêm nhiễu\n","        new_population.append(child)\n","\n","    population = new_population\n","    training_times.append(time.time() - start_time)\n","\n","    # Lưu trạng thái cá thể tốt nhất\n","    if (gen + 1) % log_frequency == 0:\n","        torch.save(best_policy_state, os.path.join(save_dir, f\"Generation_{gen + 1}.pt\"))\n","        np.savez_compressed(os.path.join(save_dir,f'results.npz'),\n","                            rewards = all_best_rewards, training_times = training_times)\n","\n","        # Chơi game và lưu video\n","        best_policy = NeuralNetwork(num_actions, device).to(device)\n","        best_policy.load_state_dict(best_policy_state)\n","        best_policy.eval()\n","        total_step, total_reward = play(best_policy, env_id, device, seed, max_steps, video_dir)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}